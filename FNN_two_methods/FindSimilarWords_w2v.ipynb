{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json_lines as jls\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_file:  contents_small.jl\n",
      "weight_dir:  Weights_w2v\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \" \"\n",
    "if TRAIN == \"test\":\n",
    "    train_file = 'contents_test.jl'\n",
    "    weight_dir = 'Weights_w2v_small'\n",
    "else:\n",
    "    train_file = 'contents_small.jl'\n",
    "    weight_dir = 'Weights_w2v'\n",
    "print(\"train_file: \", train_file)\n",
    "print(\"weight_dir: \", weight_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id = np.load(f'{weight_dir}/dict_w2v.npy',allow_pickle=True).item()\n",
    "id_to_word = {}\n",
    "for key in word_to_id:\n",
    "    id_to_word[word_to_id[key]] = key\n",
    "print(len(word_to_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* test model's prediction ability below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learning'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 24>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m cache[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mz\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cache\n\u001B[1;32m---> 24\u001B[0m learning \u001B[38;5;241m=\u001B[39m one_hot_encode(\u001B[43mword_to_id\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlearning\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, \u001B[38;5;28mlen\u001B[39m(word_to_id),s\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     25\u001B[0m result \u001B[38;5;241m=\u001B[39m forward(model, np\u001B[38;5;241m.\u001B[39masarray([learning]), return_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m (id_to_word[\u001B[38;5;28mid\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i, \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(np\u001B[38;5;241m.\u001B[39margsort(result)[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m10\u001B[39m):\n",
      "\u001B[1;31mKeyError\u001B[0m: 'learning'"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model[\"w1\"] = np.load(f'{weight_dir}/weight_1_10.npy')\n",
    "model[\"w2\"] = np.load(f'{weight_dir}/weight_2_10.npy')\n",
    "def one_hot_encode(id, vocab_size, s=False):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    if s==True:\n",
    "        print(len(res))\n",
    "    return res\n",
    "def softmax(X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x - np.max(x))\n",
    "        res.append(exp / exp.sum())\n",
    "    return res\n",
    "def forward(model, X, return_cache=True):\n",
    "    cache = {}\n",
    "    cache[\"a1\"] = X @ model[\"w1\"]\n",
    "    cache[\"a2\"] = cache[\"a1\"] @ model[\"w2\"]\n",
    "    cache[\"z\"] = softmax(cache[\"a2\"])\n",
    "    if not return_cache:\n",
    "        return cache[\"z\"]\n",
    "    return cache\n",
    "learning = one_hot_encode(word_to_id[\"learning\"], len(word_to_id),s=True)\n",
    "result = forward(model, np.asarray([learning]), return_cache=False)[0]\n",
    "for word in (id_to_word[id] for i, id in enumerate(np.argsort(result)[::-1]) if i < 10):\n",
    "    print(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 10)\n"
     ]
    }
   ],
   "source": [
    "Vectors = np.load(f'{weight_dir}/weight_1_10.npy')\n",
    "print(Vectors.shape)\n",
    "def ed(m, n):\n",
    " return np.sqrt(np.sum((m - n) ** 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impressed', 'sophistication', 'disabling', 'pm', 'shillings', 'ski', 'kenyan', 'hinder', 'efficiency', 'rounds', 'services']\n",
      "['cultural', 'sindh', 'area', 'singaporeans', 'tend', 'kenyan', 'vaccinated', 'discouraging', 'campaign', 'non', 'sugar']\n",
      "['not', 'sindh', 'tend', 'shillings', 'discouraging', 'ski', 'mobility', 'disabling', 'protected', 'imported', \"agency's\"]\n",
      "['hebei', 'make', 'unforgettable', 'kenyan', 'applauded', 'pte', 'borders', 'eritrea', 'hub', 'friday', 'rounds']\n",
      "['high', 'sindh', 'disabling', 'shillings', 'someone', 'exporters', 'houngbo', 'maintenance', 'discouraging', 'imported', 'facing']\n",
      "['places', 'sindh', 'shillings', 'kenyan', 'applauded', 'eritrea', 'tend', 'discouraging', 'imported', 'unrest', 'published']\n",
      "['she', 'rounds', 'building', 'houngbo', 'involved', 'first', 'pm', 'sophistication', 'likely', 'disabling', 'exporters']\n",
      "['singapore', 'make', 'maintenance', 'vaccinated', 'discouraging', 'eritrea', 'borders', 'non', 'experts', 'melts', 'concertini']\n",
      "['community', 'tend', 'maintenance', 'threatening', 'discouraging', 'services', 'shillings', 'eritrea', 'hinder', 'likely', 'promoting']\n",
      "['expressed', 'kenyan', 'ski', 'although', 'make', 'unforgettable', 'hub', 'eritrea', 'discouraging', 'vaccinated', 'suffering']\n",
      "['said', 'shillings', 'sindh', 'discouraging', 'tend', 'maintenance', 'ski', 'threatening', 'mobility', 'eritrea', 'hinder']\n",
      "['scenery', 'shillings', 'kenyan', 'ski', 'unforgettable', 'make', 'sindh', 'possible', 'should', 'carton', 'vaccinated']\n",
      "['ceremony', 'sophistication', 'likely', 'sindh', 'threatening', 'shillings', 'first', 'discouraging', 'pm', 'promoting', 'hinder']\n",
      "['addressing', 'sindh', 'discouraging', 'hoon', 'experts', 'courtesy', 'ski', 'unrest', 'suffering', \"agency's\", 'kenyan']\n",
      "['hemisphere', 'likely', 'eritrea', 'concertini', 'hinder', 'threatening', 'shillings', 'sophistication', 'hoon', 'melts', 'applauded']\n",
      "['valuable', 'melts', 'applauded', 'concertini', 'lee', 'singaporeans', 'maintenance', 'likely', 'paloma', 'sugar', 'every']\n",
      "['full', 'sindh', 'protected', 'north', \"agency's\", 'imported', 'kenyan', 'building', 'shillings', 'first', 'sophistication']\n",
      "['her', 'maintenance', 'hinder', 'services', 'sophistication', 'likely', 'melts', 'oral', 'pm', 'expanding', 'leaders']\n",
      "['intangible', 'sindh', 'kenyan', \"agency's\", 'maintenance', 'unrest', 'designation', 'campaign', 'ski', 'reported', 'pte']\n",
      "['zhe', 'applauded', 'maintenance', 'tend', 'likely', 'hinder', 'shillings', 'protected', 'eritrea', 'melts', 'lee']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    words = []\n",
    "    ID = int(np.random.uniform(0,169))\n",
    "    # print(\"WORD: \",id_to_word[ID])\n",
    "    words.append(id_to_word[ID])\n",
    "    VEC = Vectors[ID]\n",
    "    dist_to_id = {}\n",
    "    dists = []\n",
    "    for i, vector in enumerate(Vectors):\n",
    "        dist = ed(vector,VEC)\n",
    "        dist_to_id[dist]=i\n",
    "        dists.append(dist)\n",
    "    dists.sort(reverse=True)\n",
    "    for i, dist in enumerate(dists):\n",
    "        if i == 10:\n",
    "            break\n",
    "        words.append(id_to_word[dist_to_id[dist]])\n",
    "    print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}